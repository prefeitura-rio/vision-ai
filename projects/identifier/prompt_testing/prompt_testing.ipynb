{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e176cff-5f5f-48a9-b8ed-4a4638860834",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import base64\n",
    "import datetime\n",
    "import io\n",
    "import json\n",
    "import sys\n",
    "import textwrap\n",
    "from pathlib import Path\n",
    "from typing import List, Union\n",
    "\n",
    "import basedosdados as bd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import google.generativeai as genai\n",
    "from langchain import PromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, chat_models\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from IPython.display import Markdown\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "sys.path.insert(0, \"../../\")\n",
    "from utils_sheets import save_data_in_sheets\n",
    "\n",
    "bd.config.billing_project_id = 'rj-escritorio-dev'\n",
    "bd.config.from_file = True\n",
    "bd.__version__\n",
    "\n",
    "\n",
    "pd.options.display.max_columns = 999\n",
    "pd.options.display.max_rows = 1999\n",
    "pd.options.display.max_colwidth = 200\n",
    "\n",
    "\n",
    "from vision_ai_api import APIVisionAI\n",
    "\n",
    "import vertexai\n",
    "from vertexai.preview.generative_models import GenerativeModel, Part\n",
    "# GOOGLE_API_KEY = \"\"\n",
    "PROJECT_ID = \"rj-escritorio-dev\"\n",
    "LOCATION = \"us-central1\"\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7075b49-2121-45b9-915c-495cf89e2ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"secrets.json\") as f:\n",
    "    s = json.load(f)\n",
    "\n",
    "\n",
    "vision_ai_api = APIVisionAI(\n",
    "    username=s.get(\"username\"),\n",
    "    password=s.get(\"password\"),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66448f1e-84c5-4fc4-9318-79fe9cf4cf42",
   "metadata": {},
   "source": [
    "\n",
    "- storage images: https://console.cloud.google.com/storage/browser/datario-public/flooding_detection?project=datario\n",
    "- imagens figma: https://www.figma.com/file/Qv89NLopXS60Lqf3XfTZiN/Untitled?type=design&node-id=3-44&mode=design&t=3a4g8D4QLiDQ8f3i-0\n",
    "- langchain ref: https://python.langchain.com/docs/integrations/platforms/google\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9204531c-8ca9-4c97-a767-d125233a3587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_link_from_storage():\n",
    "    dataset_id = \"flooding_detection\"\n",
    "    table_id = \"classified_images\"\n",
    "    st = bd.Storage(dataset_id=dataset_id, table_id=table_id)\n",
    "    blobs = (\n",
    "        st.client[\"storage_staging\"]\n",
    "        .bucket(\"datario-public\")\n",
    "        .list_blobs(prefix=f\"{dataset_id}/{table_id}\")\n",
    "    )\n",
    "\n",
    "    url_list = []\n",
    "    for blob in blobs:\n",
    "        url = str(blob.public_url)\n",
    "        if \".\" in url.split(\"/\")[-1]:\n",
    "            url_list.append(url)\n",
    "    return url_list\n",
    "\n",
    "\n",
    "def get_urls_and_labels():\n",
    "    urls = get_image_link_from_storage()\n",
    "    labels = [\n",
    "        {\"path\": \"images_with_label/flood\", \"label\": True, \"object\": \"alagamento\"},\n",
    "        {\"path\": \"images_with_label/no_flood\", \"label\": False, \"object\": \"alagamento\"},\n",
    "    ]\n",
    "    data = []\n",
    "    for url in urls:\n",
    "        for item in labels:\n",
    "            if item.get(\"path\") in str(url):\n",
    "                data.append(\n",
    "                    {\n",
    "                        \"object\": item.get(\"object\"),\n",
    "                        \"label\": item.get(\"label\"),\n",
    "                        \"image_url\": url,\n",
    "                    }\n",
    "                )\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "def balance_and_sample(df, N):\n",
    "    # Get the minimum count of the two labels\n",
    "    min_count = min(df[\"label\"].value_counts())\n",
    "\n",
    "    # Balance the DataFrame\n",
    "    df_balanced = pd.concat(\n",
    "        [df[df[\"label\"] == True].head(min_count), df[df[\"label\"] == False].head(min_count)]\n",
    "    )\n",
    "    df_balanced = df_balanced.sample(frac=1).reset_index(drop=True)\n",
    "    # Sample N rows\n",
    "    if N > len(df_balanced):\n",
    "        print(\n",
    "            f\"Requested number of samples ({N}) is more than the available balanced dataset size ({len(df_balanced)}).\"\n",
    "        )\n",
    "        return df_balanced\n",
    "    return df_balanced.head(N)\n",
    "\n",
    "\n",
    "def get_image_from_url(image_url):\n",
    "    response = requests.get(image_url)\n",
    "    img = Image.open(io.BytesIO(response.content))\n",
    "    img.thumbnail((640, 480))\n",
    "    return img\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54135f28-635a-41df-b78c-f27dc18f9572",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, chat_models\n",
    "\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "\n",
    "class Object(BaseModel):\n",
    "    object: str = Field(description=\"The object from the objects\")\n",
    "    label_explanation: str = Field(\n",
    "        description=\"Highly detailed visual description of the image given the object context\"\n",
    "    )\n",
    "    label: Union[bool, str, None] = Field(\n",
    "        description=\"Label indicating the condition or characteristic of the object\"\n",
    "    )\n",
    "\n",
    "\n",
    "class ObjectFactory:\n",
    "    @classmethod\n",
    "    def generate_sample(cls) -> Object:\n",
    "        return Object(\n",
    "            object=\"<Object from objects>\",\n",
    "            label_explanation=\"<Visual description of the image given the object context>\",\n",
    "            label=\"<Selected label from objects>\",\n",
    "        )\n",
    "\n",
    "\n",
    "class Output(BaseModel):\n",
    "    objects: List[Object]\n",
    "\n",
    "\n",
    "class OutputFactory:\n",
    "    @classmethod\n",
    "    def generate_sample(cls) -> Output:\n",
    "        return Output(objects=[ObjectFactory.generate_sample()])\n",
    "\n",
    "\n",
    "class OutputFactory:\n",
    "    @classmethod\n",
    "    def generate_sample(cls) -> Output:\n",
    "        return Output(objects=[ObjectFactory.generate_sample()])\n",
    "\n",
    "\n",
    "def get_parser():\n",
    "\n",
    "    # Create the output parser using the Pydantic model\n",
    "    output_parser = PydanticOutputParser(pydantic_object=Output)\n",
    "\n",
    "    # Valid JSON string\n",
    "    output_example_str = str(OutputFactory().generate_sample().dict()).replace(\"'\", '\"')\n",
    "\n",
    "    output_example_str = textwrap.dedent(output_example_str)\n",
    "    output_example = output_parser.parse(output_example_str)\n",
    "    output_example_parsed = json.dumps(output_example.dict(), indent=4)\n",
    "\n",
    "    output_schema = json.loads(output_parser.pydantic_object.schema_json())\n",
    "    output_schema_parsed = json.dumps(output_schema, indent=4)\n",
    "\n",
    "    return output_parser, output_schema_parsed, output_example_parsed\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def gemini_pro_vision_langchain(\n",
    "    image_url, prompt, max_output_token=300, temperature=0.4, top_k=32, top_p=1\n",
    "):\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-pro-vision\",\n",
    "        google_api_key=GOOGLE_API_KEY,\n",
    "        max_output_token=max_output_token,\n",
    "        temperature=temperature,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    content = [\n",
    "        {\"type\": \"text\", \"text\": prompt},\n",
    "        {\"type\": \"image_url\", \"image_url\": image_url},\n",
    "    ]\n",
    "    message = HumanMessage(content=content)\n",
    "    return llm.invoke([message])\n",
    "\n",
    "\n",
    "def gemini_pro_vision_google(\n",
    "    image_url, prompt, max_output_token=300, temperature=0.4, top_k=32, top_p=1\n",
    "):\n",
    "    image_response = requests.get(image_url)\n",
    "    image = Image.open(io.BytesIO(image_response.content))\n",
    "    genai.configure(api_key=GOOGLE_API_KEY)\n",
    "    model = genai.GenerativeModel(\"gemini-pro-vision\")\n",
    "    responses = model.generate_content(\n",
    "        contents=[prompt, image],\n",
    "        generation_config={\n",
    "            \"max_output_tokens\": max_output_token,\n",
    "            \"temperature\": temperature,\n",
    "            \"top_k\": top_k,\n",
    "            \"top_p\": top_p,\n",
    "        },\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    responses.resolve()\n",
    "    ai_response = responses.text\n",
    "    return ai_response\n",
    "\n",
    "\n",
    "def gemini_pro_vision_vertex(\n",
    "    image_url, prompt, max_output_token=300, temperature=0.4, top_k=32, top_p=1\n",
    "):\n",
    "    image_response = requests.get(image_url)\n",
    "    image_type = image_url.split(\".\")[-1]\n",
    "    model = GenerativeModel(\"gemini-pro-vision\")\n",
    "    responses = model.generate_content(\n",
    "        contents=[\n",
    "            prompt,\n",
    "            Part.from_data(image_response.content, f\"image/{image_type}\")\n",
    "        ],\n",
    "        generation_config={\n",
    "            \"max_output_tokens\": max_output_token,\n",
    "            \"temperature\": temperature,\n",
    "            \"top_k\": top_k,\n",
    "            \"top_p\": top_p,\n",
    "        },\n",
    "    )\n",
    "    ai_response = responses.text\n",
    "    return ai_response\n",
    "\n",
    "\n",
    "def predict_and_save(\n",
    "    save_data,\n",
    "    output_parser,\n",
    "    image_url,\n",
    "    prompt,\n",
    "    max_output_token,\n",
    "    temperature,\n",
    "    top_k,\n",
    "    top_p,\n",
    "    experiment_name,\n",
    "    experiment_datetime,\n",
    "    predictions_path,\n",
    "):\n",
    "    response = gemini_pro_vision_vertex(\n",
    "        image_url=image_url,\n",
    "        prompt=prompt,\n",
    "        max_output_token=max_output_token,\n",
    "        temperature=temperature,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        response_parsed = output_parser.parse(response)\n",
    "        response_parsed = response_parsed.dict()\n",
    "    except Exception as e:\n",
    "        response_parsed = response\n",
    "\n",
    "    save_data_in_sheets(\n",
    "        save_data=save_data,\n",
    "        data={\n",
    "            \"prompt\": prompt,\n",
    "            \"max_output_token\": max_output_token,\n",
    "            \"temperature\": temperature,\n",
    "            \"top_k\": top_k,\n",
    "            \"top_p\": top_p,\n",
    "            \"experiment_name\": experiment_name,\n",
    "            \"experiment_datetime\": experiment_datetime,\n",
    "            \"true_object\": \"\",\n",
    "            \"response\": response_parsed,\n",
    "            \"image_url\": image_url,\n",
    "            \"image\": f'=IMAGE(\"{image_url}\")',\n",
    "        },\n",
    "        data_url=\"https://docs.google.com/spreadsheets/d/122uOaPr8YdW5PTzrxSPF-FD0tgco596HqgB7WK7cHFw/edit#gid=436224340\",\n",
    "        prompt_url=\"https://docs.google.com/spreadsheets/d/122uOaPr8YdW5PTzrxSPF-FD0tgco596HqgB7WK7cHFw/edit#gid=1779223884\",\n",
    "    )\n",
    "\n",
    "    pd.DataFrame([{\"image_url\": image_url}]).to_csv(\n",
    "        path_or_buf=predictions_path,\n",
    "        index=False,\n",
    "        header=not predictions_path.exists(),\n",
    "        mode=\"a\",\n",
    "    )\n",
    "\n",
    "    return response, response_parsed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cfe0abf-d0cb-4994-b137-78642a66cbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explode_df(dataframe, column_to_explode, prefix=None):\n",
    "    df = dataframe.copy()\n",
    "    exploded_df = df.explode(column_to_explode)\n",
    "    new_df = pd.json_normalize(exploded_df[column_to_explode])\n",
    "\n",
    "    if prefix:\n",
    "        new_df = new_df.add_prefix(f\"{prefix}_\")\n",
    "\n",
    "    df.drop(columns=column_to_explode, inplace=True)\n",
    "    new_df.index = exploded_df.index\n",
    "    result_df = df.join(new_df)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "def get_objetcs_labels_df(objects, keep_null=False):\n",
    "    objects_df = objects.rename(columns={\"id\": \"object_id\"})\n",
    "    objects_df = objects_df[[\"name\", \"object_id\", \"labels\"]]\n",
    "    labels = explode_df(objects_df, \"labels\")\n",
    "    if not keep_null:\n",
    "        labels = labels[~labels[\"value\"].isin([\"null\"])]\n",
    "    labels = labels.rename(columns={\"label_id\": \"label\"})\n",
    "    labels = labels.reset_index(drop=True)\n",
    "    return labels\n",
    "\n",
    "with open(\"secrets.json\") as f:\n",
    "    s = json.load(f)\n",
    "\n",
    "vision_api = APIVisionAI(\n",
    "    username=s.get(\"username\"),\n",
    "    password=s.get(\"password\"),\n",
    ")\n",
    "\n",
    "\n",
    "def get_prompt_template():\n",
    "    objects = pd.DataFrame(vision_api._get_all_pages(path=\"/objects\"))\n",
    "    labels = get_objetcs_labels_df(objects, keep_null=True)\n",
    "\n",
    "    data = vision_api._get_all_pages(\n",
    "            path=\"/prompts\"\n",
    "        )\n",
    "    prompt_parameters = data[0]\n",
    "    prompt_text = prompt_parameters.get(\"prompt_text\")\n",
    "    prompt_objects = prompt_parameters.get(\"objects\")\n",
    "\n",
    "    selected_labels_cols = [\"name\", \"criteria\", \"identification_guide\", \"value\"]\n",
    "    labels = labels[selected_labels_cols]\n",
    "    labels = labels[labels[\"name\"].isin(prompt_objects)]\n",
    "    labels = labels.rename(columns={\"name\": \"object\", \"value\": \"label\"})\n",
    "    objects_table_md = labels.to_markdown(index=False)\n",
    "    # prompt_text =  prompt_text.replace(\"{objects_table_md}\", objects_table_md)\n",
    "    \n",
    "    \n",
    "    prompt_parameters['prompt_text'] = prompt_text\n",
    "    prompt_parameters['objects_table_md'] = objects_table_md\n",
    "    \n",
    "    return prompt_parameters\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_urls_from_path(url_path):\n",
    "    urls = get_image_link_from_storage()\n",
    "    data = []\n",
    "    for url in urls:\n",
    "        if url_path in str(url):\n",
    "            data.append(\n",
    "                {\n",
    "                    \"image_url\": url,\n",
    "                }\n",
    "            )\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def get_objects_table_from_sheets(\n",
    "    url: str = \"https://docs.google.com/spreadsheets/d/122uOaPr8YdW5PTzrxSPF-FD0tgco596HqgB7WK7cHFw/edit#gid=1672006844\",\n",
    "):\n",
    "    request_url = url.replace(\"edit#gid=\", \"export?format=csv&gid=\")\n",
    "    response = requests.get(request_url)\n",
    "    dataframe = pd.read_csv(io.StringIO(response.content.decode(\"utf-8\")), dtype=str)\n",
    "    dataframe[\"label\"] = dataframe[\"label\"].fillna(\"null\")\n",
    "    dataframe = dataframe[dataframe['use'] == '1']\n",
    "    dataframe = dataframe.drop(columns=['use'])\n",
    "    \n",
    "    objects_table_md = dataframe.to_markdown(index=False)\n",
    "    \n",
    "    \n",
    "    objects_labels = (\n",
    "        dataframe[[\"object\", \"label\"]]\n",
    "        .groupby(by=[\"object\"], sort=False)[\"label\"]\n",
    "        .apply(lambda x: \", \".join(x))\n",
    "        .reset_index()\n",
    "    )\n",
    "    objects_labels[\"label\"] = objects_labels[\"label\"].str.replace(\n",
    "        \"true, false\", \"bool\"\n",
    "    )\n",
    "\n",
    "    objects_labels_md = objects_labels.to_markdown(index=False)\n",
    "    objects_labels_md = objects_labels_md\n",
    "    return objects_table_md, objects_labels_md\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31a9690-fa27-44fe-a9fd-c2bef1f8546e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf4e698-9a4a-4c53-93a3-f3d8ce29a3ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1400caf-50f1-45c5-965d-1ad59e209b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"e25cabfd-6861-47a8-8ff1-3e3efb34ca57\",\n",
      "  \"name\": \"base\",\n",
      "  \"model\": \"gemini-pro-vision\",\n",
      "  \"prompt_text\": \"\\nRole: Urban Road Image Analyst\\n\\nObjective: Analyze CCTV images from Rio de Janeiro, focusing on road-related elements to assess urban road conditions. Ignore trees in the analyze.\\n\\n\\n----\\n\\nInput:\\n\\n- A CCTV image.\\n\\n----\\n\\nProcess:\\n\\n1.  Image Condition\\n\\n    - Begin by closely examine the image, paying special attention to any areas with uniform color or distortions. Uniform color patches may indicate missing data or image corruption.\\n    - Based on your observations, assign an appropriate label to the 'image_condition'. Your choice should reflect the observed quality of the image, considering the aspects of uniform color areas, distortions, and blurring.\\n    \\n2. Image Description:\\n\\n    - Proceed to provide an unbiased and detailed image_description, focusing on visible aspects for further analisys. Use at least 500 words!\\n    - Ensure that this description is purely based on what is visible in the image and not influenced by external contexts or assumptions.\\n\\n3. Analysis and Classification:\\n\\n    - Use the provided image_description to classify each object uniquely using the 'Objects Table'. \\n    - Each classification should be supported with clear, observable evidence.\\n\\n4. Output: \\n\\n    - Present findings in a structured JSON format as specified in the output schema. \\n    - This format is essential for inputting data into an API.\\n\\n----\\n\\nOutput:\\n\\n- Format the output as a JSON instance following the provided schema. \\n\\nOutput Schema:\\n\\n\\n```json\\n{output_schema}\\n```\\n\\nExample Output:\\n\\n- For each object analyzed, your output should resemble the following format:\\n\\n\\n```json\\n{output_example}\\n```\\n\\n\\n----\\n\\n\\nObjects Table: Use the following objects and their associated criteria and identification_guide to determine the appropriate label for each aspect of the image. Each object label_explanation must be in your own words, reflecting a deep understanding and interpretation of the image. Use at least 300 words for each label_explanation!\\n\\n{objects_table_md}\\n\\n\",\n",
      "  \"max_output_token\": 2048,\n",
      "  \"temperature\": 0.2,\n",
      "  \"top_k\": 32,\n",
      "  \"top_p\": 1.0,\n",
      "  \"objects\": [\n",
      "    \"image_description\",\n",
      "    \"road_blockade\",\n",
      "    \"water_in_road\",\n",
      "    \"traffic_ease_vehicle\",\n",
      "    \"image_condition\",\n",
      "    \"water_level\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def get_prompt(prompt_parameters,prompt_template=None, objects_table_md=None):    \n",
    "    \n",
    "    \n",
    "    if not prompt_template:\n",
    "        prompt_template = prompt_parameters.get('prompt_text')    \n",
    "        \n",
    "    _, output_schema, output_example = get_parser()\n",
    "    \n",
    "    if not objects_table_md:\n",
    "        objects_table_md = prompt_parameters.get('objects_table_md')\n",
    "    \n",
    "    \n",
    "    filled_prompt = (\n",
    "        prompt_template.replace(\"                        \", \"\")\n",
    "        .replace(\"{objects_table_md}\",objects_table_md)\n",
    "        .replace(\"{output_schema}\", output_schema)\n",
    "        .replace(\"{output_example}\", output_example)\n",
    "    )\n",
    "\n",
    "    return filled_prompt, prompt_template\n",
    "\n",
    "prompt_text_local = \"\"\"\n",
    "Role: Urban Road Image Analyst\n",
    "\n",
    "Objective: Analyze CCTV images from Rio de Janeiro, focusing on road-related elements to assess urban road conditions. Ignore trees in the analyze.\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "Input:\n",
    "\n",
    "- A CCTV image.\n",
    "\n",
    "----\n",
    "\n",
    "Process:\n",
    "\n",
    "1.  Image Condition\n",
    "\n",
    "    - Begin by closely examine the image, paying special attention to any areas with uniform color or distortions. Uniform color patches may indicate missing data or image corruption.\n",
    "    - Based on your observations, assign an appropriate label to the 'image_condition'. Your choice should reflect the observed quality of the image, considering the aspects of uniform color areas, distortions, and blurring.\n",
    "    \n",
    "2. Image Description:\n",
    "\n",
    "    - Proceed to provide an unbiased and detailed image_description, focusing on visible aspects for further analisys. Use at least 500 words!\n",
    "    - Ensure that this description is purely based on what is visible in the image and not influenced by external contexts or assumptions.\n",
    "\n",
    "3. Analysis and Classification:\n",
    "\n",
    "    - Use the provided image_description to classify each object uniquely using the 'Objects Table'. \n",
    "    - Each classification should be supported with clear, observable evidence.\n",
    "\n",
    "4. Output: \n",
    "\n",
    "    - Present findings in a structured JSON format as specified in the output schema. \n",
    "    - This format is essential for inputting data into an API.\n",
    "\n",
    "----\n",
    "\n",
    "Output:\n",
    "\n",
    "- Format the output as a JSON instance following the provided schema. \n",
    "\n",
    "Output Schema:\n",
    "\n",
    "\n",
    "```json\n",
    "{output_schema}\n",
    "```\n",
    "\n",
    "Example Output:\n",
    "\n",
    "- For each object analyzed, your output should resemble the following format:\n",
    "\n",
    "\n",
    "```json\n",
    "{output_example}\n",
    "```\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "Objects Table: Use the following objects and their associated criteria and identification_guide to determine the appropriate label for each aspect of the image. Each object label_explanation must be in your own words, reflecting a deep understanding and interpretation of the image. Use at least 300 words for each label_explanation!\n",
    "\n",
    "{objects_table_md}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# PUT PROMPT\n",
    "prompt_parameters = vision_ai_api._get('/prompts').get('items')[0]\n",
    "prompt_id = prompt_parameters.get('id')\n",
    "request_body = {\n",
    "  \"name\": prompt_parameters.get(\"name\"),\n",
    "  \"model\": prompt_parameters.get(\"model\"),\n",
    "  \"prompt_text\": prompt_text_local,\n",
    "  \"max_output_token\": prompt_parameters.get(\"max_output_token\"),\n",
    "  \"temperature\": prompt_parameters.get(\"temperature\"),\n",
    "  \"top_k\": prompt_parameters.get(\"top_k\"),\n",
    "  \"top_p\": prompt_parameters.get(\"top_p\")\n",
    "}\n",
    "\n",
    "\n",
    "r = vision_ai_api._put(\n",
    "    path=f'/prompts/{prompt_id}',\n",
    "    json_data=request_body\n",
    ")\n",
    "print(json.dumps(r.json(),  indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ae5124-de4b-4257-aea2-958a7fc8ff58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07634c9a-85e1-4010-82aa-eec8bbf3a5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = get_urls_from_path(url_path=\"images_predicted_as_flood\")\n",
    "objects_table_md, objects_labels_md = get_objects_table_from_sheets()\n",
    "df = get_urls_from_path(url_path=\"/\")\n",
    "# prompt_parameters = get_prompt_template()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17e3693d-cf6e-4988-98c3-4503db2aba2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"test-object-label-table\"\n",
    "experiment_datetime = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "max_output_token = prompt_parameters.get(\"max_output_token\")\n",
    "temperature = prompt_parameters.get(\"temperature\")\n",
    "top_k = prompt_parameters.get(\"top_k\")\n",
    "top_p = prompt_parameters.get(\"top_p\")\n",
    "retry = 5\n",
    "\n",
    "prompt, prompt_template = get_prompt(\n",
    "    prompt_parameters,\n",
    "    prompt_template=prompt_text_local,\n",
    "    objects_table_md=objects_table_md\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdf4d670-030f-46c3-ae73-fc53f02032e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "Role: Urban Road Image Analyst\n",
       "\n",
       "Objective: Analyze CCTV images from Rio de Janeiro, focusing on road-related elements to assess urban road conditions. Ignore trees in the analyze.\n",
       "\n",
       "\n",
       "----\n",
       "\n",
       "Input:\n",
       "\n",
       "- A CCTV image.\n",
       "\n",
       "----\n",
       "\n",
       "Process:\n",
       "\n",
       "1.  Image Condition\n",
       "\n",
       "    - Begin by closely examine the image, paying special attention to any areas with uniform color or distortions. Uniform color patches may indicate missing data or image corruption.\n",
       "    - Based on your observations, assign an appropriate label to the 'image_condition'. Your choice should reflect the observed quality of the image, considering the aspects of uniform color areas, distortions, and blurring.\n",
       "    \n",
       "2. Image Description:\n",
       "\n",
       "    - Proceed to provide an unbiased and detailed image_description, focusing on visible aspects for further analisys. Use at least 500 words!\n",
       "    - Ensure that this description is purely based on what is visible in the image and not influenced by external contexts or assumptions.\n",
       "\n",
       "3. Analysis and Classification:\n",
       "\n",
       "    - Use the provided image_description to classify each object uniquely using the 'Objects Table'. \n",
       "    - Each classification should be supported with clear, observable evidence.\n",
       "\n",
       "4. Output: \n",
       "\n",
       "    - Present findings in a structured JSON format as specified in the output schema. \n",
       "    - This format is essential for inputting data into an API.\n",
       "\n",
       "----\n",
       "\n",
       "Output:\n",
       "\n",
       "- Format the output as a JSON instance following the provided schema. \n",
       "\n",
       "Output Schema:\n",
       "\n",
       "\n",
       "```json\n",
       "{\n",
       "    \"$defs\": {\n",
       "        \"Object\": {\n",
       "            \"properties\": {\n",
       "                \"object\": {\n",
       "                    \"description\": \"The object from the objects\",\n",
       "                    \"title\": \"Object\",\n",
       "                    \"type\": \"string\"\n",
       "                },\n",
       "                \"label_explanation\": {\n",
       "                    \"description\": \"Highly detailed visual description of the image given the object context\",\n",
       "                    \"title\": \"Label Explanation\",\n",
       "                    \"type\": \"string\"\n",
       "                },\n",
       "                \"label\": {\n",
       "                    \"anyOf\": [\n",
       "                        {\n",
       "                            \"type\": \"boolean\"\n",
       "                        },\n",
       "                        {\n",
       "                            \"type\": \"string\"\n",
       "                        },\n",
       "                        {\n",
       "                            \"type\": \"null\"\n",
       "                        }\n",
       "                    ],\n",
       "                    \"description\": \"Label indicating the condition or characteristic of the object\",\n",
       "                    \"title\": \"Label\"\n",
       "                }\n",
       "            },\n",
       "            \"required\": [\n",
       "                \"object\",\n",
       "                \"label_explanation\",\n",
       "                \"label\"\n",
       "            ],\n",
       "            \"title\": \"Object\",\n",
       "            \"type\": \"object\"\n",
       "        }\n",
       "    },\n",
       "    \"properties\": {\n",
       "        \"objects\": {\n",
       "            \"items\": {\n",
       "                \"$ref\": \"#/$defs/Object\"\n",
       "            },\n",
       "            \"title\": \"Objects\",\n",
       "            \"type\": \"array\"\n",
       "        }\n",
       "    },\n",
       "    \"required\": [\n",
       "        \"objects\"\n",
       "    ],\n",
       "    \"title\": \"Output\",\n",
       "    \"type\": \"object\"\n",
       "}\n",
       "```\n",
       "\n",
       "Example Output:\n",
       "\n",
       "- For each object analyzed, your output should resemble the following format:\n",
       "\n",
       "\n",
       "```json\n",
       "{\n",
       "    \"objects\": [\n",
       "        {\n",
       "            \"object\": \"<Object from objects>\",\n",
       "            \"label_explanation\": \"<Visual description of the image given the object context>\",\n",
       "            \"label\": \"<Selected label from objects>\"\n",
       "        }\n",
       "    ]\n",
       "}\n",
       "```\n",
       "\n",
       "\n",
       "----\n",
       "\n",
       "\n",
       "Objects Table: Use the following objects and their associated criteria and identification_guide to determine the appropriate label for each aspect of the image. Each object label_explanation must be in your own words, reflecting a deep understanding and interpretation of the image. Use at least 300 words for each label_explanation!\n",
       "\n",
       "| object               | label             | criteria                                                              | identification_guide                                                                                                                                                                                                                                                                                               |\n",
       "|:---------------------|:------------------|:----------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
       "| image_condition      | clean             | Instructions already provided.                                        | Instructions already provided.                                                                                                                                                                                                                                                                                     |\n",
       "| image_condition      | poor              | Instructions already provided.                                        | Instructions already provided.                                                                                                                                                                                                                                                                                     |\n",
       "| image_description    | null              | Instructions already provided.                                        | Instructions already provided.                                                                                                                                                                                                                                                                                     |\n",
       "| water_in_road        | false             | No water present on the road; the road is completely dry.             | Look for dry road surface. A dry road should show no signs of moisture, including the absence of puddles, wet spots, and any kind of reflections commonly associated with water.                                                                                                                                   |\n",
       "| water_in_road        | true              | Presence of water covering  the road.                                 | Identify areas where water is visibly covering the road surface or slightly wet. Look for consistent, widespread reflections.                                                                                                                                                                                      |\n",
       "| water_level          | low_indifferent   | No or minimal water on the road's surface.                            | Inspect for dry or slightly wet road. Minimal water is indicated by isolated, small or shallow puddles.  Look for areas with shaddows.                                                                                                                                                                             |\n",
       "| water_level          | puddle            | Water covers a significant part of the road.                          | Look for areas where water significantly covers the road. with widespread reflections.                                                                                                                                                                                                                             |\n",
       "| water_level          | flodding          | Water completely covers the road's surface.                           | Identify areas where water completely submerges the road. Visible cues include a lack of visible road surface due to water coverage.                                                                                                                                                                               |\n",
       "| traffic_ease_vehicle | easy              | Minimal or no water on the road.                                      | Look for clear, dry, or slightly wet road surfaces and small, manageable puddles that do not interfere with traffic                                                                                                                                                                                                |\n",
       "| traffic_ease_vehicle | moderate          | Presence of significant, larger puddles                               | Detect larger puddles that could cause minor traffic disruptions but are still navigable for most vehicles.                                                                                                                                                                                                        |\n",
       "| traffic_ease_vehicle | difficult         | A partial portion of the road is covered with medium water level      | Identify areas where water coverage is extensive and high, causing notable hindrance to vehicle.                                                                                                                                                                                                                   |\n",
       "| traffic_ease_vehicle | impossibe         | Complete submergence of the road with high water level                | Identify a scenarios where the road is entirely submerged/flooded, making it completely impassable for vehicles.                                                                                                                                                                                                   |\n",
       "| road_blockade        | free              | The road is clear, without any obstructions impeding traffic flow.    | Confirm the road is free from obstructions. Normal flow of cars, bicycles, and other vehicles indicates a 'free' road label. Natural water bodies (canals, rivers, lakes, etc) adjacent to but not obstructing the road also indicate a clear path. Trees alongside or near the road indicates a 'free' road label |\n",
       "| road_blockade        | partially_blocked | Obstructions present that partially impede traffic but allow passage. | Look for indications of partial road coverage by any elements that slow trafic flow. Partial road coverage by water should be considered.                                                                                                                                                                          |\n",
       "| road_blockade        | totally_blocked   | The road is completely inaccessible due to obstructions.              | Identify any significant obstructions like extensive water flooding that make the road entirely impassable, offering no alternative paths.                                                                                                                                                                         |\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7ff0322-8617-44ef-a382-af56ff8858e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10950"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b47a46-398f-4114-85d4-684752ccac6b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions_path = Path(f\"./data/predictions/{experiment_name}__{experiment_datetime}.csv\")\n",
    "\n",
    "if predictions_path.exists():\n",
    "    predictions = pd.read_csv(predictions_path)\n",
    "    predictions_list = predictions[\"image_url\"].tolist()\n",
    "else:\n",
    "    predictions_list = []\n",
    "\n",
    "predictions_list = []\n",
    "output_parser, output_schema, output_example = get_parser()\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    image_url = row[\"image_url\"]\n",
    "    retry_count = 0\n",
    "    while retry_count <= retry:\n",
    "        try:\n",
    "            if image_url not in predictions_list:\n",
    "                _response, response_parsed = predict_and_save(\n",
    "                    save_data=True,\n",
    "                    output_parser=output_parser,\n",
    "                    image_url=image_url,\n",
    "                    prompt=prompt,\n",
    "                    max_output_token=max_output_token,\n",
    "                    temperature=temperature,\n",
    "                    top_k=top_k,\n",
    "                    top_p=top_p,\n",
    "                    experiment_name=experiment_name,\n",
    "                    experiment_datetime=experiment_datetime,\n",
    "                    predictions_path=predictions_path,\n",
    "                )\n",
    "\n",
    "                print(f\"{index} - {len(df)}\")\n",
    "                # print(json.dumps(response_parsed, indent=4))\n",
    "                # display(get_image_from_url(image_url))\n",
    "            else:\n",
    "                print(f\"{index} - {len(df)}: already predicted\")\n",
    "\n",
    "            retry_count = retry + 1\n",
    "        except Exception as e:\n",
    "            retry_count += 1\n",
    "            print(f\"{index} - {len(df)}: Error.. Retrying:{retry_count}\\n {e}\\n\\n\\n\\nAI Response:\")\n",
    "            print(_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aad9663-727a-4620-8525-f2803bde51e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4512007-5a73-426e-a2a8-59e2997881b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3defb987-2795-43f8-ac61-93e34206fc47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7149fe35-3aed-4d2c-84d5-4fe74b35ebbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb829be-4a45-4d8d-bc88-42b486c7b9ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2617a8eb-d39f-43e2-8aab-4b662f39e4b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6f0ff8-d34f-486e-b839-00df334ef541",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d529d0-afa9-4d11-9a84-063e6764492f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Objects Tables to Markdown\n",
    "\n",
    "# prompt_parameters = vision_ai_api._get('/prompts').get('items')[0]\n",
    "# df = get_objects_table_from_sheets()\n",
    "\n",
    "# markdown_str = \"\"\n",
    "# for object_name in df['object'].unique().tolist():\n",
    "#     labels_df = df[df['object']==object_name]\n",
    "#     i = 0\n",
    "#     for _, row   in labels_df.iterrows():\n",
    "#         if i==0:\n",
    "#             markdown_str += f\"\\nObject: {object_name}\\n\"\n",
    "        \n",
    "#         markdown_str+=f\"\"\"\n",
    "#             - Label: {row['labels']}: \n",
    "#                 - Criteria: {row['criteria']}\n",
    "#                 - Identification guide: {row['identification_guide']}\n",
    "#         \"\"\".replace(\"            \",\"\")\n",
    "        \n",
    "        \n",
    "#         i+=1\n",
    "\n",
    "# markdown_str = markdown_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1ef0b8-bbeb-4a8c-8573-9fe97ae08527",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1399960-9aec-4073-a795-48c1bfdc3667",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36947ea5-f032-4ce9-9743-bdf0a9fde682",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05adb4b9-a7e7-4a2b-927d-0d38a5466434",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1145a2-8260-46ec-bb03-b9dcd859ebe1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d04e2a6-ecdb-4b86-b125-1c35aae3b916",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6aac9f08-33c6-4a33-8d2c-a7ffadbf2b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel(\"gemini-pro-vision\")\n",
    "model.generate_content()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1b4c78-5a41-4a3d-8154-2b49f3ad24d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6efc556-4562-4bfa-8ef8-7c894a234990",
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ad79bb0e-a989-48ec-9e20-7840794439eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 8.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 8.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 16.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96\n",
      "97\n",
      "98\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "camera_id = \"001477\"\n",
    "N = 100\n",
    "for i, image_url in enumerate(N * [f\"https://storage.googleapis.com/datario-public/vision-ai/staging/{camera_id}.png\"]):\n",
    "    retry_count = 0\n",
    "    while retry_count <= retry:\n",
    "        try:\n",
    "            response, response_parsed = predict_and_save(\n",
    "                save_data=True, \n",
    "                output_parser=output_parser, \n",
    "                image_url=image_url, \n",
    "                prompt=prompt, \n",
    "                max_output_token=max_output_token, \n",
    "                temperature=temperature, \n",
    "                top_k=top_k, \n",
    "                top_p=top_p\n",
    "            )\n",
    "\n",
    "            print(f\"{i}\")\n",
    "            # print(json.dumps(response_parsed, indent=4))\n",
    "            # display(get_image_from_url(image_url))\n",
    "            retry_count = retry + 1\n",
    "        except Exception as e:\n",
    "            retry_count += 1\n",
    "            print(f\"{i}: Error.. Retrying:{retry_count}\\n {e}\\n\\n\\n\\nAI Response:\")\n",
    "            print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3084e9e8-3a53-49fc-a34c-fab80aa570f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91a81b6-0840-4c0c-b3b7-b287a408f738",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45de45f6-5f96-4bd0-889f-1c77de15e247",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2de343-2021-4b9d-bdec-5fad2713b942",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c6d823-1be5-41b4-a30a-e8775a8fff20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d617e71e-a3d1-45c3-bf59-3dbad561fad2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d42160-cc20-4afe-b88e-cfe5675e3b29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6936a0-ded2-4ae8-8817-64fcde85bffc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6ae79b-fb54-404a-a7d6-4c8889282e04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5df7d4d6-e96e-41fe-9385-346a74d9398c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1955"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(response_parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "db35215f-8f6d-4b8a-98f3-5c7ef1c7ff5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Output(objects=[Object(object='image_description', label_explanation='A night view of a flooded city street with cars driving through the water. The street is lit by streetlights and the headlights of the cars. The image is clear and not blurry.', label='null'), Object(object='road_blockade', label_explanation='There is a large puddle of water on the road, but it is still passable for vehicles.', label='partially_blocked'), Object(object='road_blockade_reason', label_explanation='The road is partially blocked by a large puddle of water.', label='water_puddle'), Object(object='water_in_road', label_explanation='There is a large puddle of water on the road.', label='true'), Object(object='traffic_ease_vehicle', label_explanation='The road is partially blocked by a large puddle of water, but it is still passable for most vehicles.', label='moderate'), Object(object='image_condition', label_explanation='The image is clear and not blurry.', label='clean'), Object(object='brt_lane', label_explanation='There is no BRT lane visible in the image.', label='false'), Object(object='landslide', label_explanation='There is no landslide visible in the image.', label='false'), Object(object='fire', label_explanation='There is no fire visible in the image.', label='false'), Object(object='inside_tunnel', label_explanation='The image is not taken inside a tunnel.', label='false'), Object(object='building_collapse', label_explanation='There is no building collapse visible in the image.', label='false'), Object(object='alert_category', label_explanation='There is a large puddle of water on the road, but it is still passable for most vehicles. There are no other major issues visible in the image.', label='minor')])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser.parse(response_parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15e012c-8c4e-42d6-a915-9c04de0ae0c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f459c94-045e-477d-9e2d-4daae12c3a54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a557a65b-d976-43d2-ae29-fadba8336f32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e416e7-f770-4758-80c1-616e788e4d80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1b1675-9ed5-41a3-96de-ea07b5a17cb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153ad56f-48c0-493c-8010-2d477f75e015",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a93eef-6414-40ca-a260-05b363514053",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c4d874-13f5-4fc5-9535-2ceb4f35c6a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363487d0-abf1-4810-bd4e-af6ecdc9bd0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ef2392-2b12-4b5f-adbb-d7ac8808a183",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0981861-bf78-4313-9707-78fbeed07676",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095f41b9-72f9-4ff4-a92b-90467f0d6985",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa25eb26-86d0-4fbd-baed-32eea8413c96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8eb834-0fda-43a1-a48d-38ccfc620cb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f249ce22-8953-425f-b048-3799f4132320",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f633618-b120-46a9-bebf-571115518247",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a583ac5-28e6-4172-8254-87fcbba4dcbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34af26c7-c932-44fc-8ff6-06bfd9449586",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63777b1b-9c46-4d4e-81e9-02f8434ce218",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320dc68c-282d-42e0-8560-d7efd730678d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8311f0e-d371-431d-87bf-a12cb150c6f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680f66e1-9636-4e51-9818-a74ac476259a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5aaa57-c42b-4a9d-a641-2fac9f04a490",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e727eb-a617-4360-863e-aeb2b16af3bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8072c17-f764-4091-98bd-e9a7e04ecb58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948454c4-478a-42b2-8888-c0ee53604edd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c131bba-ce3d-4166-b29b-da2209dfa26b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65073a69-14c4-429e-8f54-77216854c06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import seaborn as sns  \n",
    "\n",
    "\n",
    "true_values = df_final['flood'].tolist()\n",
    "predicted_values = df_final['ai_label'].tolist()\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(true_values, predicted_values)\n",
    "precision = precision_score(true_values, predicted_values, pos_label=True)\n",
    "recall = recall_score(true_values, predicted_values, pos_label=True)\n",
    "f1 = f1_score(true_values, predicted_values, pos_label=True)\n",
    "conf_matrix = confusion_matrix(true_values, predicted_values)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "\n",
    "cm = confusion_matrix(true_values, predicted_values)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['False', 'True'], yticklabels=['False', 'True'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# resize_factor = 3\n",
    "#imgs = 10\n",
    "# time = 1:00\n",
    "# Accuracy: 0.7\n",
    "# Precision: 0.6666666666666666\n",
    "# Recall: 0.5\n",
    "# F1 Score: 0.5714285714285715\n",
    "\n",
    "# resize_factor = 1\n",
    "#imgs = 10\n",
    "# time = 1:15\n",
    "# Accuracy: 0.8\n",
    "# Precision: 0.75\n",
    "# Recall: 0.75\n",
    "# F1 Score: 0.75\n",
    "\n",
    "\n",
    "# resize_factor = 5\n",
    "#imgs = 10\n",
    "# time = 1:20\n",
    "# Accuracy: 0.8\n",
    "# Precision: 0.75\n",
    "# Recall: 0.75\n",
    "# F1 Score: 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc24eb2-79a9-4008-a62d-68352822cfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['miss'] = np.where(df_final['flood']==df_final['ai_label'],False , True)\n",
    "mask = df_final['miss']==True\n",
    "miss = df_final[mask]\n",
    "miss_imgs = miss['base64'].tolist()\n",
    "miss_ai_labels = miss['ai_label'].tolist()\n",
    "\n",
    "\n",
    "for base64_image, ai_label in zip(miss_imgs, miss_ai_labels):\n",
    "    print(f'AI classyfy as: {ai_label}')\n",
    "    display_img(base64_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d77779a-d488-4f08-8aea-52394f9741e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219ef146-77b9-4969-bce0-c27dc606059b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b32695b-c8a0-46d8-a5e4-2121f88ac158",
   "metadata": {},
   "source": [
    "You are a highly skilled prompt engineering focus in create prompts for CCTV image recognition. Your task is to optimize and refine a provided example prompt. \n",
    "\n",
    "you output is a diff between the provided prompt and the new optmized prompt\n",
    "\n",
    "shall we start?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2b74ec-93a9-4328-862f-54c3f1162e12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Escritorio de Dados - Default",
   "language": "python",
   "name": "default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
